{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Musat_Bianca_P1_dummy_dataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Latend Dirichlet Allocation (LDA) - dummy dataset\n",
        "\n",
        "Musat Bianca-Stefania\n",
        "\n",
        "407 Artificial Intelligence"
      ],
      "metadata": {
        "id": "96SFcy6oKtTa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 300,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xo6bNrifoGB2",
        "outputId": "c97c0d89-83da-462f-986b-bef681e8dd03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Requirement already satisfied: pymc in /usr/local/lib/python3.7/dist-packages (2.3.8)\n"
          ]
        }
      ],
      "source": [
        "# import all necessary packages\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import download\n",
        "download('stopwords')\n",
        "download('wordnet')\n",
        "download('punkt')\n",
        "\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "!pip install pymc\n",
        "import pymc as pm\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rcParams\n",
        "from pymc3 import traceplot\n",
        "from math import log, exp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transform string data from dummy dataset into numbers so that it can be used by LDA\n",
        "\n",
        "We will define a vocabulary which is a dictionary that maps each word to a number. We also define an index_to_word dictionary which allows us to easily trace back each word from the associated number."
      ],
      "metadata": {
        "id": "FTMVu8Y9LbbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [[\"aaa\", \"bbb\", \"aaa\"], [\"bbb\", \"aaa\", \"bbb\"], [\"aaa\", \"bbb\", \"bbb\", \"aaa\"], [\"uuu\", \"vvv\"], [\"uuu\", \"vvv\", \"vvv\"], [\"uuu\", \"vvv\", \"vvv\", \"uuu\"]]"
      ],
      "metadata": {
        "id": "5ViQ8weaoMbB"
      },
      "execution_count": 301,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NO_FILES = 3"
      ],
      "metadata": {
        "id": "DzQa9VlwfPF-"
      },
      "execution_count": 302,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_vocab(dataset):\n",
        "    vocabulary = {}  # dict {word : index}\n",
        "    index_to_word = {}  # dict {index : word}\n",
        "    index = 0\n",
        "\n",
        "    for item in dataset:\n",
        "        for word in item:\n",
        "            if word not in vocabulary:\n",
        "                vocabulary[word] = index\n",
        "                index_to_word[index] = word\n",
        "                index += 1\n",
        "    return vocabulary, index_to_word"
      ],
      "metadata": {
        "id": "gEPYAho_LMGI"
      },
      "execution_count": 303,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab, index_to_word = make_vocab(dataset)\n",
        "print(\"Vocabulary length: \", len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZ59O-uoLQTz",
        "outputId": "e4ed4811-f53f-49bb-ffec-8e75d41d86cf"
      },
      "execution_count": 304,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary length:  4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_dataset(dataset):  # transform dataset into number representation\n",
        "    for i, item in enumerate(dataset):\n",
        "        for j, word in enumerate(item):\n",
        "            if word in vocab:\n",
        "                dataset[i][j] = vocab[word]\n",
        "            else:  # if word not in vocabulary, assign new index\n",
        "                dataset[i][j] = len(vocab)"
      ],
      "metadata": {
        "id": "ZluSyjEtLQpS"
      },
      "execution_count": 305,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_dataset(dataset)"
      ],
      "metadata": {
        "id": "x7mRGdilLQra"
      },
      "execution_count": 306,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dataset: \", dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pit5BnyhLQ1V",
        "outputId": "e0358958-03d8-47ea-ae05-12bd7054cace"
      },
      "execution_count": 307,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset:  [[0, 1, 0], [1, 0, 1], [0, 1, 1, 0], [2, 3], [2, 3, 3], [2, 3, 3, 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LDA\n",
        "\n",
        "In the next section I have implemented and described the LDA model.\n",
        "\n",
        "LDA is a probabilistic topic model in which each document in the dataset exhibits multiple topics in different proportions, each topic being a distribution over the predefined vocabulary.\n",
        "\n",
        "LDA assumes that the documents are created via a generative process that looks like this: we first choose a distribution over topics for each document, and for each word in that document we choose a random topic from that distribution and then we choose a word from the correct distribution over the vocabulary. As stated before, this generative process will lead to documents belonging to multiple topics in different proportions.\n",
        "\n",
        "LDA reverse engeneer this process. In order to do that, it has to draw a topic for each word in each document, and then to draw the real word from the correct distribution (which is the distribution associated with the topic). The distribution that we use to draw the topic/word is Categorical distribution. The parameters used for these 2 distributions are going to be drawn from a Dirichlet distribution, as this is the conjugate prior for the Categorical.\n",
        "\n",
        "z[m,n] = Categorical(theta[m])  # drawing a topic for each word (each word n in each document m)\n",
        "\n",
        "w[m,n] = Categorical(phi[z[m,n]])   # drawing the physical word (each word n in each document m)\n",
        "\n",
        "theta[m] = Dirichlet(alpha)  # topic distribution for document m (it will be K dimensional, as there ar K topics in total)\n",
        "\n",
        "phi[k] = Dirichlet(beta)  # word distribution for topic k (it will be V dimentional, as there are V possible words in the dataset)"
      ],
      "metadata": {
        "id": "RN2R6FJYLoDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "K = 2  # number of categories/topics\n",
        "M = len(dataset)  # number of documents in the dataset\n",
        "V = len(vocab)  # vocabulary length"
      ],
      "metadata": {
        "id": "YsIh4dahL382"
      },
      "execution_count": 308,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lda(dataset, alpha=np.ones(K), beta=np.ones(V)):\n",
        "    Nm = []  # number of words in each document\n",
        "    for d in dataset:\n",
        "      Nm.append(len(d))\n",
        "\n",
        "    # draw word distribution for each topic ( dimentional, as there are V possible words in the dataset)\n",
        "    phi1 = [pm.Dirichlet(\"phi1_%i\" % i, theta=beta) for i in range(K)]\n",
        "    phi = [pm.CompletedDirichlet(\"phi_%i\" % i, phi1[i]) for i in range(K)]\n",
        "    phi = pm.Container(phi)\n",
        "\n",
        "    # draw topic distribution for each document (K dimensional, as there ar K topics in total)\n",
        "    theta1 = [pm.Dirichlet(\"theta1_%i\" % i, theta=alpha) for i in range(M)]\n",
        "    theta = [pm.CompletedDirichlet(\"theta_%i\" % i, theta1[i]) for i in range(M)]\n",
        "    theta = pm.Container(theta)\n",
        "\n",
        "    rand_topic = []  # randomly choose a topic for each word in each document\n",
        "    for i in range(M):\n",
        "      rand_topic.append(np.random.randint(K, size=Nm[i]))\n",
        "\n",
        "    # draw a topic for each word in each document\n",
        "    Z = [pm.Categorical(\"Z1_%i\" % i, p=theta[i], size=Nm[i], value=rand_topic[i]) for i in range(M)]\n",
        "    Z = pm.Container(Z)\n",
        "\n",
        "    # draw the word itself from the word distribution\n",
        "    W = [pm.Categorical(\"w_%d_%i\" % (d,i), p = pm.Lambda(\"z_%d_%i\" % (d,i), lambda z=Z[d][i], phi=phi : phi[z]), value=dataset[d][i], observed=True) for d in range(M) for i in range(Nm[d])]\n",
        "    W = pm.Container(W)\n",
        "\n",
        "    # create LDA model\n",
        "    model = pm.Model([theta, phi, Z, W, phi1, theta1])\n",
        "\n",
        "    # sample values\n",
        "    mcmc = pm.MCMC(model)\n",
        "    trace = mcmc.sample(iter=100000, burn=8000)\n",
        "  \n",
        "    print(\"\\nThe topic of each word in each document:\\n\")\n",
        "    for m in range(M):  \n",
        "      tr = mcmc.trace('Z1_%i' % m)[100000 - 8000 - 1]\n",
        "      print(tr)\n",
        "\n",
        "    print(\"\\nThe mean topic of each word in each document:\\n\")\n",
        "    for m in range(M):  \n",
        "      tr = mcmc.trace('Z1_%i' % m)[-1000:-1].mean(axis=0)\n",
        "      print(tr)\n",
        "    \n",
        "    # for m in range(M):  \n",
        "    #   traceplot(mcmc.trace('Z1_%i'% m )[-1000:-1].mean(axis=0))\n",
        "\n",
        "    print(\"\\nThe distribution of words for each topic:\\n\")\n",
        "    for k in range(K):\n",
        "      print(\"Topic \", k)\n",
        "      for i, j in enumerate(mcmc.trace('phi_%i' % k)[-1000:-1].mean(axis=0)[0]):\n",
        "          print(\"   \", index_to_word[i], \" \", j)\n",
        "      print()\n",
        "\n",
        "    print(\"\\nTopic distribution for each document:\\n\")\n",
        "    for m in range(M):\n",
        "      print(\"Document \", m)\n",
        "      for i, j in enumerate(mcmc.trace('theta_%i' % m)[-1000:-1].mean(axis=0)[0]):\n",
        "          print(\"Topic \", i, \" \", j)\n",
        "      print()\n",
        "\n",
        "    return (theta, phi, Z, W)\n"
      ],
      "metadata": {
        "id": "liVxkamtoOow"
      },
      "execution_count": 309,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choosing alpha and beta\n",
        "\n",
        "Alpha and beta parameters are fixed and it is therefore important to choose them carefully. They affect the sparcity of the model in the following way:\n",
        "\n",
        "- alpha controls the document topic density, so we want a higher alpha when we have more topics\n",
        "- beta controls word topic density, so we want a higher beta when the number of words in vocabulary is high\n",
        "\n",
        "In our case, we have a small number of topics, so we would rather choose a small alpha, and a small number of words in the dictionary, so we should also choose a small beta. However, I have tested a number of values and the results are presented below."
      ],
      "metadata": {
        "id": "iSEJBbESV8NO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choosing a way to measure model success\n",
        "\n",
        "As a measure of success, I have chosen to implement the accuracy per topic. So, I am choosing the correct topic for a document as being the most frequent topic among those documents that should belong to the same topic. Then I compute how many words have been correctly assign to that topic."
      ],
      "metadata": {
        "id": "fmILrJQXhAve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the accuracy for each topic\n",
        "def topic_acc(Z_matrix):\n",
        "    indx = 1\n",
        "    counters = []\n",
        "    maxims = []\n",
        "    lenghts = []\n",
        "    print()\n",
        "    for lst in Z_matrix:\n",
        "        counters.append(Counter(lst))  # for each topic, we count how many words have been assign to it\n",
        "        maxims.append(max(lst, key=lst.count))  # for each document, we get the most common topic\n",
        "        lenghts.append(len(lst))  # for each document, we get the number of words in it\n",
        "        if indx % NO_FILES == 0:\n",
        "            acc = 0\n",
        "            tot = 0\n",
        "            i = 0\n",
        "            for c in counters:\n",
        "                acc += c[max(maxims, key=maxims.count)] / lenghts[i]  # compute accuracy as correctly assigned words / total number of words\n",
        "                i += 1\n",
        "            \n",
        "            print(\"Topic: \", max(maxims, key=maxims.count))\n",
        "            print(\"Topic accuracy \", acc / NO_FILES)\n",
        "            print(\"Most frequent topic in each document belonging to the real topic: \", maxims)\n",
        "            print()\n",
        "            counters = []\n",
        "            maxims = []\n",
        "            lenghts = []\n",
        "        indx += 1"
      ],
      "metadata": {
        "id": "7jXoFF_PZoMM"
      },
      "execution_count": 310,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(theta, phi, Z, W) = lda(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PKiCbOYNw5R",
        "outputId": "8005d764-c727-47bd-8317-3c5bca7f5e7b"
      },
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pymc/MCMC.py:81: UserWarning: Instantiating a Model object directly is deprecated. We recommend passing variables directly to the Model subclass.\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " [-----------------100%-----------------] 100000 of 100000 complete in 268.2 sec\n",
            "The topic of each word in each document:\n",
            "\n",
            "[0 1 0]\n",
            "[1 1 0]\n",
            "[0 0 0 0]\n",
            "[1 1]\n",
            "[1 1 1]\n",
            "[1 1 1 1]\n",
            "\n",
            "The mean topic of each word in each document:\n",
            "\n",
            "[0.2972973  0.44644645 0.30530531]\n",
            "[0.17517518 0.16816817 0.1031031 ]\n",
            "[0.22822823 0.1951952  0.14414414 0.13513514]\n",
            "[0.85285285 0.71771772]\n",
            "[0.78878879 0.66066066 0.64664665]\n",
            "[0.91291291 0.73573574 0.76376376 0.96196196]\n",
            "\n",
            "The distribution of words for each topic:\n",
            "\n",
            "Topic  0\n",
            "    aaa   0.444378329368263\n",
            "    bbb   0.34191350152389244\n",
            "    uuu   0.08089344832459579\n",
            "    vvv   0.13281472078324993\n",
            "\n",
            "Topic  1\n",
            "    aaa   0.12901981217318448\n",
            "    bbb   0.1541212380998981\n",
            "    uuu   0.40693435843014636\n",
            "    vvv   0.3099245912967735\n",
            "\n",
            "\n",
            "Topic distribution for each document:\n",
            "\n",
            "Document  0\n",
            "Topic  0   0.5939133417974944\n",
            "Topic  1   0.4060866582025048\n",
            "\n",
            "Document  1\n",
            "Topic  0   0.7241807521855379\n",
            "Topic  1   0.2758192478144637\n",
            "\n",
            "Document  2\n",
            "Topic  0   0.7013438461163137\n",
            "Topic  1   0.29865615388368416\n",
            "\n",
            "Document  3\n",
            "Topic  0   0.33461423580336\n",
            "Topic  1   0.6653857641966431\n",
            "\n",
            "Document  4\n",
            "Topic  0   0.36750086772976437\n",
            "Topic  1   0.6324991322702357\n",
            "\n",
            "Document  5\n",
            "Topic  0   0.24499005982953342\n",
            "Topic  1   0.7550099401704651\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Z_matrix = []\n",
        "for i in range(len(Z)):\n",
        "        Z_matrix.append(list(Z[i].value))\n",
        "print(Z_matrix)\n",
        "\n",
        "topic_acc(Z_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqt3NZFEhVbo",
        "outputId": "b729b187-36c4-4abd-e960-1eae03341f48"
      },
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, 1, 0], [1, 1, 0], [0, 0, 0, 0], [1, 1], [1, 1, 1], [1, 1, 1, 1]]\n",
            "\n",
            "Topic:  0\n",
            "Topic accuracy  0.6666666666666666\n",
            "Most frequent topic in each document belonging to the real topic:  [0, 1, 0]\n",
            "\n",
            "Topic:  1\n",
            "Topic accuracy  1.0\n",
            "Most frequent topic in each document belonging to the real topic:  [1, 1, 1]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(theta, phi, Z, W) = lda(dataset, alpha=[0.2] * K, beta=[0.3] * V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IO3buiezN1-w",
        "outputId": "57613d37-8f69-42f5-de83-90c2d3ca8e5f"
      },
      "execution_count": 311,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pymc/MCMC.py:81: UserWarning: Instantiating a Model object directly is deprecated. We recommend passing variables directly to the Model subclass.\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " [-----------------100%-----------------] 100000 of 100000 complete in 290.7 sec\n",
            "The topic of each word in each document:\n",
            "\n",
            "[0 0 0]\n",
            "[0 0 0]\n",
            "[0 0 0 0]\n",
            "[1 1]\n",
            "[1 1 1]\n",
            "[1 1 1 1]\n",
            "\n",
            "The mean topic of each word in each document:\n",
            "\n",
            "[0.       0.       0.001001]\n",
            "[0.         0.06206206 0.        ]\n",
            "[0. 0. 0. 0.]\n",
            "[1. 1.]\n",
            "[1. 1. 1.]\n",
            "[1. 1. 1. 1.]\n",
            "\n",
            "The distribution of words for each topic:\n",
            "\n",
            "Topic  0\n",
            "    aaa   0.3976424269316546\n",
            "    bbb   0.5825710053279558\n",
            "    uuu   0.01364369361280558\n",
            "    vvv   0.006142874127584022\n",
            "\n",
            "Topic  1\n",
            "    aaa   0.04182444854307025\n",
            "    bbb   0.003965125213279359\n",
            "    uuu   0.45598808015235265\n",
            "    vvv   0.4982223460912992\n",
            "\n",
            "\n",
            "Topic distribution for each document:\n",
            "\n",
            "Document  0\n",
            "Topic  0   0.9479150468977848\n",
            "Topic  1   0.05208495310221343\n",
            "\n",
            "Document  1\n",
            "Topic  0   0.7920700012086869\n",
            "Topic  1   0.2079299987913135\n",
            "\n",
            "Document  2\n",
            "Topic  0   0.9695469976022981\n",
            "Topic  1   0.030453002397686443\n",
            "\n",
            "Document  3\n",
            "Topic  0   0.019716163310200697\n",
            "Topic  1   0.9802838366897882\n",
            "\n",
            "Document  4\n",
            "Topic  0   0.01164243475938886\n",
            "Topic  1   0.9883575652406101\n",
            "\n",
            "Document  5\n",
            "Topic  0   0.04999545960934983\n",
            "Topic  1   0.9500045403906489\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Z_matrix = []\n",
        "for i in range(len(Z)):\n",
        "        Z_matrix.append(list(Z[i].value))\n",
        "print(Z_matrix)\n",
        "\n",
        "topic_acc(Z_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08pdODi9hWu7",
        "outputId": "78b48f39-ac00-4a5b-c4bb-b33313c18c4a"
      },
      "execution_count": 312,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [1, 1], [1, 1, 1], [1, 1, 1, 1]]\n",
            "\n",
            "Topic:  0\n",
            "Topic accuracy  1.0\n",
            "Most frequent topic in each document belonging to the real topic:  [0, 0, 0]\n",
            "\n",
            "Topic:  1\n",
            "Topic accuracy  1.0\n",
            "Most frequent topic in each document belonging to the real topic:  [1, 1, 1]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(theta, phi, Z, W) = lda(dataset, alpha=[0.11] * K, beta=[0.2] * V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhscSPUcN6F5",
        "outputId": "1ca4cb6e-5d62-4c6b-f730-76b3c4d6f344"
      },
      "execution_count": 276,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pymc/MCMC.py:81: UserWarning: Instantiating a Model object directly is deprecated. We recommend passing variables directly to the Model subclass.\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " [-----------------100%-----------------] 100000 of 100000 complete in 282.7 sec\n",
            "The topic of each word in each document:\n",
            "\n",
            "[1 1 1]\n",
            "[0 1 0]\n",
            "[1 0 0 1]\n",
            "[1 1]\n",
            "[1 1 1]\n",
            "[1 1 1 1]\n",
            "\n",
            "The mean topic of each word in each document:\n",
            "\n",
            "[1. 1. 1.]\n",
            "[0.13613614 0.58358358 0.11011011]\n",
            "[0.19319319 0.         0.003003   0.22522523]\n",
            "[1. 1.]\n",
            "[1. 1. 1.]\n",
            "[1.         0.48748749 1.         1.        ]\n",
            "\n",
            "The distribution of words for each topic:\n",
            "\n",
            "Topic  0\n",
            "    aaa   0.10719313820046873\n",
            "    bbb   0.8923190397965621\n",
            "    uuu   1.6779658124231744e-08\n",
            "    vvv   0.0004878052232935155\n",
            "\n",
            "Topic  1\n",
            "    aaa   0.32524827840477616\n",
            "    bbb   0.10315131781138397\n",
            "    uuu   6.865887305025884e-05\n",
            "    vvv   0.57153174491079\n",
            "\n",
            "\n",
            "Topic distribution for each document:\n",
            "\n",
            "Document  0\n",
            "Topic  0   0.03309916349938243\n",
            "Topic  1   0.9669008365006161\n",
            "\n",
            "Document  1\n",
            "Topic  0   0.6886695904472939\n",
            "Topic  1   0.31133040955270425\n",
            "\n",
            "Document  2\n",
            "Topic  0   0.8736883971227626\n",
            "Topic  1   0.12631160287723267\n",
            "\n",
            "Document  3\n",
            "Topic  0   7.068809352801602e-06\n",
            "Topic  1   0.9999929311906474\n",
            "\n",
            "Document  4\n",
            "Topic  0   0.005224329033954616\n",
            "Topic  1   0.9947756709660588\n",
            "\n",
            "Document  5\n",
            "Topic  0   0.16887125379713303\n",
            "Topic  1   0.8311287462028646\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Z_matrix = []\n",
        "for i in range(len(Z)):\n",
        "        Z_matrix.append(list(Z[i].value))\n",
        "print(Z_matrix)\n",
        "\n",
        "topic_acc(Z_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuPzDnAwhXf3",
        "outputId": "27c94053-8963-41f9-bd6a-5668ced61cd1"
      },
      "execution_count": 277,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 1, 1], [0, 1, 0], [1, 0, 0, 1], [1, 1], [1, 1, 1], [1, 1, 1, 1]]\n",
            "\n",
            "Topic:  1\n",
            "Topic accuracy  0.611111111111111\n",
            "Most frequent topic in each document belonging to the real topic:  [1, 0, 1]\n",
            "\n",
            "Topic:  1\n",
            "Topic accuracy  1.0\n",
            "Most frequent topic in each document belonging to the real topic:  [1, 1, 1]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(theta, phi, Z, W) = lda(dataset, alpha=[0.5] * K, beta=[0.6] * V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lr6U162EoQNo",
        "outputId": "15052d20-784f-4208-d449-6922e15bd16d"
      },
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pymc/MCMC.py:81: UserWarning: Instantiating a Model object directly is deprecated. We recommend passing variables directly to the Model subclass.\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " [-----------------100%-----------------] 100000 of 100000 complete in 271.6 sec\n",
            "The topic of each word in each document:\n",
            "\n",
            "[1 1 1]\n",
            "[1 1 1]\n",
            "[1 1 1 1]\n",
            "[0 1]\n",
            "[0 0 0]\n",
            "[0 0 0 0]\n",
            "\n",
            "The mean topic of each word in each document:\n",
            "\n",
            "[0.97797798 0.97697698 0.99299299]\n",
            "[0.91291291 0.96696697 0.88388388]\n",
            "[0.99099099 0.99399399 0.95495495 1.        ]\n",
            "[0.         0.05705706]\n",
            "[0.03503504 0.16716717 0.05905906]\n",
            "[0.         0.05305305 0.02702703 0.        ]\n",
            "\n",
            "The distribution of words for each topic:\n",
            "\n",
            "Topic  0\n",
            "    aaa   0.049579271991671774\n",
            "    bbb   0.07111838109067514\n",
            "    uuu   0.5070316119035984\n",
            "    vvv   0.37227073501405517\n",
            "\n",
            "Topic  1\n",
            "    aaa   0.5596940377203785\n",
            "    bbb   0.3183468933925425\n",
            "    uuu   0.03463883502896487\n",
            "    vvv   0.08732023385811269\n",
            "\n",
            "\n",
            "Topic distribution for each document:\n",
            "\n",
            "Document  0\n",
            "Topic  0   0.11693157413021617\n",
            "Topic  1   0.8830684258697818\n",
            "\n",
            "Document  1\n",
            "Topic  0   0.13195564963980375\n",
            "Topic  1   0.8680443503601752\n",
            "\n",
            "Document  2\n",
            "Topic  0   0.12513931508698276\n",
            "Topic  1   0.874860684913018\n",
            "\n",
            "Document  3\n",
            "Topic  0   0.8190633928480414\n",
            "Topic  1   0.18093660715196258\n",
            "\n",
            "Document  4\n",
            "Topic  0   0.7893482819675947\n",
            "Topic  1   0.21065171803240199\n",
            "\n",
            "Document  5\n",
            "Topic  0   0.8722924755388379\n",
            "Topic  1   0.1277075244611617\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Z_matrix = []\n",
        "for i in range(len(Z)):\n",
        "        Z_matrix.append(list(Z[i].value))\n",
        "print(Z_matrix)\n",
        "\n",
        "topic_acc(Z_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dld34iKMhYax",
        "outputId": "85f72336-b803-42eb-bb3c-351e151c53e2"
      },
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 1, 1], [1, 1, 1], [1, 1, 1, 1], [0, 1], [0, 0, 0], [0, 0, 0, 0]]\n",
            "\n",
            "Topic:  1\n",
            "Topic accuracy  1.0\n",
            "Most frequent topic in each document belonging to the real topic:  [1, 1, 1]\n",
            "\n",
            "Topic:  0\n",
            "Topic accuracy  0.8333333333333334\n",
            "Most frequent topic in each document belonging to the real topic:  [0, 0, 0]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusions task 1\n",
        "\n",
        "As we can see from the previous runs, small values of alpha and beta give perfect accuracy. As we stated before, alpha controls the document topic density so we want a small alpha because we have only 2 topics, and beta controls word topic density so we want small beta because we have only 4 words into the vocabulary. The model manage to perfectly predict the correct topic for each word."
      ],
      "metadata": {
        "id": "rHQ5iy7yw_JR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Topic-based similarity measures\n",
        "\n",
        "\n",
        "For this task I implemented a number of similarity measures proposed for LDA in these papers https://www.cs.memphis.edu/~vrus/publications/2013/CICLing-2013.RusNiraulaBanjade.pdf, https://aclanthology.org/E14-4005.pdf.\n",
        "\n",
        "I have chosen 3 similarity measures based on the distribution over topics (theta), as each document can be viewed as a distribution over topics. Thus, we can compute the similarity between 2 documents, by computing the similarity of their distributions. The 3 measures below return the dissimilarity between 2 distributions, so they will tell us how diffrent two distributions are (0 meaning identical).\n",
        "\n",
        "- Kullback-Leibler (KL) divergence -> takes two distributions p and q and computes the distance between them\n",
        "\n",
        "KL(p, q) = sum(pi * log(pi / qi)), where i defines a topic (i in [0, K))\n",
        "\n",
        "The drawbacks of KL divergence are that, if qi is 0 it is not defined, and is not symmetric.\n",
        "\n",
        "- Jensen-Shannon divergence -> takes two distributions p and q and computes the distance between them, while solving the asymmetry problem of KL by considering the average of pi and qi\n",
        "\n",
        "JS(p, q) = 1/2 * KL(p, m) + 1/2 * KL(q, m), where m = 1/2 * (p + q)\n",
        "\n",
        "- Hellinger distance -> similar to JS distance, it removes the drawback of KL divergence. An advantage of Hellinger distance is that it is very easy to compute the similarity from the distance, by substractig the result from 1.\n",
        "\n",
        "HD(p, q) = 1/2 * sqrt(sum( sqrt(pi) - sqrt(qi) )^2)"
      ],
      "metadata": {
        "id": "z1m2Jq90w5fk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kl(p, q):\n",
        "    return np.sum(p * np.log2(p / q))"
      ],
      "metadata": {
        "id": "2aAccTVS1c7d"
      },
      "execution_count": 313,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def js(p, q):\n",
        "    m = (p + q) / 2\n",
        "    return kl(p, m) / 2 + kl(q, m) / 2"
      ],
      "metadata": {
        "id": "qrWi430yGPvw"
      },
      "execution_count": 314,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hd(p, q):\n",
        "    return 1./np.sqrt(2) * np.sqrt(np.sum(np.square(np.sqrt(p) - np.sqrt(q))))"
      ],
      "metadata": {
        "id": "gIplx_OE2gyY"
      },
      "execution_count": 315,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Jensen-Shannon divergence \\n\")\n",
        "for i in range(len(theta)):\n",
        "    for j in range(len(theta)):\n",
        "        print(\"Documents\", i, j, \" have the following divergence: \", js(theta[i].value, theta[j].value))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbWswNjJ2qXt",
        "outputId": "371cf5f4-8d73-4fd5-8aa4-ce20e6df23e4"
      },
      "execution_count": 316,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jensen-Shannon divergence \n",
            "\n",
            "Documents 0 0  have the following divergence:  0.0\n",
            "Documents 0 1  have the following divergence:  0.036231314247596944\n",
            "Documents 0 2  have the following divergence:  0.03897383527850709\n",
            "Documents 0 3  have the following divergence:  0.7978568617445874\n",
            "Documents 0 4  have the following divergence:  0.7845609020531057\n",
            "Documents 0 5  have the following divergence:  0.7978756640579636\n",
            "Documents 1 0  have the following divergence:  0.036231314247596944\n",
            "Documents 1 1  have the following divergence:  0.0\n",
            "Documents 1 2  have the following divergence:  0.0002382272858704928\n",
            "Documents 1 3  have the following divergence:  0.9928723741698628\n",
            "Documents 1 4  have the following divergence:  0.9792658081338039\n",
            "Documents 1 5  have the following divergence:  0.9928914937872587\n",
            "Documents 2 0  have the following divergence:  0.03897383527850709\n",
            "Documents 2 1  have the following divergence:  0.0002382272858704928\n",
            "Documents 2 2  have the following divergence:  0.0\n",
            "Documents 2 3  have the following divergence:  0.9977194910117051\n",
            "Documents 2 4  have the following divergence:  0.98410923463249\n",
            "Documents 2 5  have the following divergence:  0.9977386143986076\n",
            "Documents 3 0  have the following divergence:  0.7978568617445874\n",
            "Documents 3 1  have the following divergence:  0.9928723741698628\n",
            "Documents 3 2  have the following divergence:  0.9977194910117051\n",
            "Documents 3 3  have the following divergence:  0.0\n",
            "Documents 3 4  have the following divergence:  0.0011916260857023754\n",
            "Documents 3 5  have the following divergence:  1.54303248450081e-08\n",
            "Documents 4 0  have the following divergence:  0.7845609020531057\n",
            "Documents 4 1  have the following divergence:  0.9792658081338039\n",
            "Documents 4 2  have the following divergence:  0.98410923463249\n",
            "Documents 4 3  have the following divergence:  0.0011916260857023754\n",
            "Documents 4 4  have the following divergence:  0.0\n",
            "Documents 4 5  have the following divergence:  0.0011973109486080936\n",
            "Documents 5 0  have the following divergence:  0.7978756640579636\n",
            "Documents 5 1  have the following divergence:  0.9928914937872587\n",
            "Documents 5 2  have the following divergence:  0.9977386143986076\n",
            "Documents 5 3  have the following divergence:  1.54303248450081e-08\n",
            "Documents 5 4  have the following divergence:  0.0011973109486080936\n",
            "Documents 5 5  have the following divergence:  0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Kullback-Leibler divergence\\n\")\n",
        "for i in range(len(theta)):\n",
        "    for j in range(len(theta)):\n",
        "        print(\"Documents\", i, j, \" have the following divergence: \", kl(theta[i].value, theta[j].value))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXhDGyXYqyXk",
        "outputId": "80fd085e-8b49-4455-ffde-377ca50aa3f8"
      },
      "execution_count": 317,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kullback-Leibler divergence\n",
            "\n",
            "Documents 0 0  have the following divergence:  0.0\n",
            "Documents 0 1  have the following divergence:  0.367103324859317\n",
            "Documents 0 2  have the following divergence:  0.5452318372905276\n",
            "Documents 0 3  have the following divergence:  11.895376768306809\n",
            "Documents 0 4  have the following divergence:  7.379276262880919\n",
            "Documents 0 5  have the following divergence:  11.935101080171762\n",
            "Documents 1 0  have the following divergence:  0.10800739628222725\n",
            "Documents 1 1  have the following divergence:  0.0\n",
            "Documents 1 2  have the following divergence:  0.0013214249550807292\n",
            "Documents 1 3  have the following divergence:  13.297296337964323\n",
            "Documents 1 4  have the following divergence:  8.40581230609105\n",
            "Documents 1 5  have the following divergence:  13.340319963069629\n",
            "Documents 2 0  have the following divergence:  0.11444644911636362\n",
            "Documents 2 1  have the following divergence:  0.0007832996853520982\n",
            "Documents 2 2  have the following divergence:  0.0\n",
            "Documents 2 3  have the following divergence:  13.319138216967556\n",
            "Documents 2 4  have the following divergence:  8.423185591843842\n",
            "Documents 2 5  have the following divergence:  13.36220111734159\n",
            "Documents 3 0  have the following divergence:  3.683529111282957\n",
            "Documents 3 1  have the following divergence:  9.7710401727355\n",
            "Documents 3 2  have the following divergence:  12.077953800040282\n",
            "Documents 3 3  have the following divergence:  0.0\n",
            "Documents 3 4  have the following divergence:  0.003576136936205326\n",
            "Documents 3 5  have the following divergence:  6.203069200412881e-08\n",
            "Documents 4 0  have the following divergence:  3.646307077139956\n",
            "Documents 4 1  have the following divergence:  9.716414733114794\n",
            "Documents 4 2  have the following divergence:  12.016851879679917\n",
            "Documents 4 3  have the following divergence:  0.01017424045944313\n",
            "Documents 4 4  have the following divergence:  0.0\n",
            "Documents 4 5  have the following divergence:  0.010295157026908894\n",
            "Documents 5 0  have the following divergence:  3.6835775857479764\n",
            "Documents 5 1  have the following divergence:  9.771106423954516\n",
            "Documents 5 2  have the following divergence:  12.078026666677474\n",
            "Documents 5 3  have the following divergence:  6.141649047516796e-08\n",
            "Documents 5 4  have the following divergence:  0.0035902437116133116\n",
            "Documents 5 5  have the following divergence:  0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Hellinger divergence\\n\")\n",
        "for i in range(len(theta)):\n",
        "    for j in range(len(theta)):\n",
        "        print(\"Documents\", i, j, \" have the following divergence: \", hd(theta[i].value, theta[j].value), \"\\t and the following similarity: \", 1 - hd(theta[i].value, theta[j].value))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hdes-U-qyZ1",
        "outputId": "3ba69d35-0941-4416-8442-38c7ca357eb1"
      },
      "execution_count": 318,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hellinger divergence\n",
            "\n",
            "Documents 0 0  have the following divergence:  0.0 \t and the following similarity:  1.0\n",
            "Documents 0 1  have the following divergence:  0.17543486658062854 \t and the following similarity:  0.8245651334193714\n",
            "Documents 0 2  have the following divergence:  0.18848741070364308 \t and the following similarity:  0.811512589296357\n",
            "Documents 0 3  have the following divergence:  0.8436390479949719 \t and the following similarity:  0.1563609520050281\n",
            "Documents 0 4  have the following divergence:  0.8184464076260862 \t and the following similarity:  0.18155359237391377\n",
            "Documents 0 5  have the following divergence:  0.8437220486411735 \t and the following similarity:  0.1562779513588265\n",
            "Documents 1 0  have the following divergence:  0.17543486658062854 \t and the following similarity:  0.8245651334193714\n",
            "Documents 1 1  have the following divergence:  0.0 \t and the following similarity:  1.0\n",
            "Documents 1 2  have the following divergence:  0.013161806225742983 \t and the following similarity:  0.986838193774257\n",
            "Documents 1 3  have the following divergence:  0.9779233537673817 \t and the following similarity:  0.02207664623261829\n",
            "Documents 1 4  have the following divergence:  0.9551952088505574 \t and the following similarity:  0.04480479114944258\n",
            "Documents 1 5  have the following divergence:  0.9779980596578586 \t and the following similarity:  0.022001940342141424\n",
            "Documents 2 0  have the following divergence:  0.18848741070364308 \t and the following similarity:  0.811512589296357\n",
            "Documents 2 1  have the following divergence:  0.013161806225742983 \t and the following similarity:  0.986838193774257\n",
            "Documents 2 2  have the following divergence:  0.0 \t and the following similarity:  1.0\n",
            "Documents 2 3  have the following divergence:  0.9873888270918181 \t and the following similarity:  0.012611172908181922\n",
            "Documents 2 4  have the following divergence:  0.9648597334649088 \t and the following similarity:  0.03514026653509117\n",
            "Documents 2 5  have the following divergence:  0.9874628641500809 \t and the following similarity:  0.012537135849919112\n",
            "Documents 3 0  have the following divergence:  0.8436390479949719 \t and the following similarity:  0.1563609520050281\n",
            "Documents 3 1  have the following divergence:  0.9779233537673817 \t and the following similarity:  0.02207664623261829\n",
            "Documents 3 2  have the following divergence:  0.9873888270918181 \t and the following similarity:  0.012611172908181922\n",
            "Documents 3 3  have the following divergence:  0.0 \t and the following similarity:  1.0\n",
            "Documents 3 4  have the following divergence:  0.031134750611073316 \t and the following similarity:  0.9688652493889267\n",
            "Documents 3 5  have the following divergence:  0.00010341994342676695 \t and the following similarity:  0.9998965800565732\n",
            "Documents 4 0  have the following divergence:  0.8184464076260862 \t and the following similarity:  0.18155359237391377\n",
            "Documents 4 1  have the following divergence:  0.9551952088505574 \t and the following similarity:  0.04480479114944258\n",
            "Documents 4 2  have the following divergence:  0.9648597334649088 \t and the following similarity:  0.03514026653509117\n",
            "Documents 4 3  have the following divergence:  0.031134750611073316 \t and the following similarity:  0.9688652493889267\n",
            "Documents 4 4  have the following divergence:  0.0 \t and the following similarity:  1.0\n",
            "Documents 4 5  have the following divergence:  0.031238145405093207 \t and the following similarity:  0.9687618545949068\n",
            "Documents 5 0  have the following divergence:  0.8437220486411735 \t and the following similarity:  0.1562779513588265\n",
            "Documents 5 1  have the following divergence:  0.9779980596578586 \t and the following similarity:  0.022001940342141424\n",
            "Documents 5 2  have the following divergence:  0.9874628641500809 \t and the following similarity:  0.012537135849919112\n",
            "Documents 5 3  have the following divergence:  0.00010341994342676695 \t and the following similarity:  0.9998965800565732\n",
            "Documents 5 4  have the following divergence:  0.031238145405093207 \t and the following similarity:  0.9687618545949068\n",
            "Documents 5 5  have the following divergence:  0.0 \t and the following similarity:  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that documents from the same class have small values for divergece, and documents from diffrent classes have high values for divergence."
      ],
      "metadata": {
        "id": "TBDWkhSL6wtc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will combine the first and last documents from the dataset to test the similarity measures implemented above. I will rerun the model and then test the dissimilarity using Jensen-Shannon divergence."
      ],
      "metadata": {
        "id": "PW-OBG_czYo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YzqDGsTw4Co",
        "outputId": "5066d774-f4b3-464b-8bb3-3a78fd13909c"
      },
      "execution_count": 324,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, 1, 0], [1, 0, 1], [0, 1, 1, 0], [2, 3], [2, 3, 3], [2, 3, 3, 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#combine 2 documents to show that the similarity measures work properly\n",
        "new_doc1 = dataset[0][:2] + dataset[-1][2:]\n",
        "new_doc2 = dataset[-1][:2] + dataset[0][2:]\n",
        "print(new_doc1)\n",
        "print(new_doc2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7QlYkDCyVeE",
        "outputId": "c5c38b07-efd8-4054-d5fa-818843f556a1"
      },
      "execution_count": 325,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 3, 2]\n",
            "[2, 3, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0] = new_doc2\n",
        "dataset[-1] = new_doc1\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6efz2DtAy1b_",
        "outputId": "dbc2f1fc-d341-4793-8fa0-0b279f8748b8"
      },
      "execution_count": 326,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2, 3, 0], [1, 0, 1], [0, 1, 1, 0], [2, 3], [2, 3, 3], [0, 1, 3, 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(theta, phi, Z, W) = lda(dataset, alpha=[0.11] * K, beta=[0.2] * V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63HxF6Vjy-ih",
        "outputId": "a9b75fb3-bb8d-4895-cda2-de1f4578b187"
      },
      "execution_count": 327,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pymc/MCMC.py:81: UserWarning: Instantiating a Model object directly is deprecated. We recommend passing variables directly to the Model subclass.\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " [-----------------100%-----------------] 100000 of 100000 complete in 281.5 sec\n",
            "The topic of each word in each document:\n",
            "\n",
            "[1 1 0]\n",
            "[0 0 0]\n",
            "[0 0 0 0]\n",
            "[1 1]\n",
            "[1 1 1]\n",
            "[0 0 1 1]\n",
            "\n",
            "The mean topic of each word in each document:\n",
            "\n",
            "[1.        1.        0.7967968]\n",
            "[0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[1. 1.]\n",
            "[1. 1. 1.]\n",
            "[0.51251251 0.17317317 1.         1.        ]\n",
            "\n",
            "The distribution of words for each topic:\n",
            "\n",
            "Topic  0\n",
            "    aaa   0.2262073365203095\n",
            "    bbb   0.7736613421256444\n",
            "    uuu   6.658652897914324e-09\n",
            "    vvv   0.00013131469539377297\n",
            "\n",
            "Topic  1\n",
            "    aaa   0.09027543570729409\n",
            "    bbb   0.06542308873346786\n",
            "    uuu   0.21113491120419534\n",
            "    vvv   0.6331665643550441\n",
            "\n",
            "\n",
            "Topic distribution for each document:\n",
            "\n",
            "Document  0\n",
            "Topic  0   0.13597369407084337\n",
            "Topic  1   0.8640263059291626\n",
            "\n",
            "Document  1\n",
            "Topic  0   0.999868035785707\n",
            "Topic  1   0.00013196421429530848\n",
            "\n",
            "Document  2\n",
            "Topic  0   0.9778011778307989\n",
            "Topic  1   0.022198822169194915\n",
            "\n",
            "Document  3\n",
            "Topic  0   0.11201119233466023\n",
            "Topic  1   0.8879888076653435\n",
            "\n",
            "Document  4\n",
            "Topic  0   2.3469208417993145e-05\n",
            "Topic  1   0.9999765307915889\n",
            "\n",
            "Document  5\n",
            "Topic  0   0.3670271142993433\n",
            "Topic  1   0.632972885700655\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Z_matrix = []\n",
        "for i in range(len(Z)):\n",
        "        Z_matrix.append(list(Z[i].value))\n",
        "print(Z_matrix)\n",
        "\n",
        "topic_acc(Z_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pN6A3Y8_zLdS",
        "outputId": "ea34d459-746f-45dd-eaa7-bd01fa725cb7"
      },
      "execution_count": 328,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 1, 0], [0, 0, 0], [0, 0, 0, 0], [1, 1], [1, 1, 1], [0, 0, 1, 1]]\n",
            "\n",
            "Topic:  0\n",
            "Topic accuracy  0.7777777777777777\n",
            "Most frequent topic in each document belonging to the real topic:  [1, 0, 0]\n",
            "\n",
            "Topic:  1\n",
            "Topic accuracy  0.8333333333333334\n",
            "Most frequent topic in each document belonging to the real topic:  [1, 1, 0]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Jensen-Shannon divergence \\n\")\n",
        "for i in range(len(theta)):\n",
        "    for j in range(len(theta)):\n",
        "        print(\"Documents\", i, j, \" have the following divergence: \", js(theta[i].value, theta[j].value))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtBJvbUazPIk",
        "outputId": "3e18997c-9960-49cc-f35a-bb7c8dd7d03a"
      },
      "execution_count": 329,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jensen-Shannon divergence \n",
            "\n",
            "Documents 0 0  have the following divergence:  0.0\n",
            "Documents 0 1  have the following divergence:  0.3737586522171188\n",
            "Documents 0 2  have the following divergence:  0.37529209660911633\n",
            "Documents 0 3  have the following divergence:  0.2516675867416454\n",
            "Documents 0 4  have the following divergence:  0.2529473924400953\n",
            "Documents 0 5  have the following divergence:  0.10693723606795216\n",
            "Documents 1 0  have the following divergence:  0.3737586522171188\n",
            "Documents 1 1  have the following divergence:  0.0\n",
            "Documents 1 2  have the following divergence:  0.00012493649016920885\n",
            "Documents 1 3  have the following divergence:  0.9967659318252986\n",
            "Documents 1 4  have the following divergence:  0.9982559040645043\n",
            "Documents 1 5  have the following divergence:  0.11068049434363715\n",
            "Documents 2 0  have the following divergence:  0.37529209660911633\n",
            "Documents 2 1  have the following divergence:  0.00012493649016920885\n",
            "Documents 2 2  have the following divergence:  0.0\n",
            "Documents 2 3  have the following divergence:  0.9984649073740739\n",
            "Documents 2 4  have the following divergence:  0.9999549608973972\n",
            "Documents 2 5  have the following divergence:  0.11198333951834734\n",
            "Documents 3 0  have the following divergence:  0.2516675867416454\n",
            "Documents 3 1  have the following divergence:  0.9967659318252986\n",
            "Documents 3 2  have the following divergence:  0.9984649073740739\n",
            "Documents 3 3  have the following divergence:  0.0\n",
            "Documents 3 4  have the following divergence:  0.0001001658548702349\n",
            "Documents 3 5  have the following divergence:  0.599875596942979\n",
            "Documents 4 0  have the following divergence:  0.2529473924400953\n",
            "Documents 4 1  have the following divergence:  0.9982559040645043\n",
            "Documents 4 2  have the following divergence:  0.9999549608973972\n",
            "Documents 4 3  have the following divergence:  0.0001001658548702349\n",
            "Documents 4 4  have the following divergence:  0.0\n",
            "Documents 4 5  have the following divergence:  0.6012986037952821\n",
            "Documents 5 0  have the following divergence:  0.10693723606795216\n",
            "Documents 5 1  have the following divergence:  0.11068049434363715\n",
            "Documents 5 2  have the following divergence:  0.11198333951834734\n",
            "Documents 5 3  have the following divergence:  0.599875596942979\n",
            "Documents 5 4  have the following divergence:  0.6012986037952821\n",
            "Documents 5 5  have the following divergence:  0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe above that the disimillarity between documents 0 and 5 (that have been mixed up) is pretty low."
      ],
      "metadata": {
        "id": "RvpsDYnc0lVX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assigning a topic to a new document\n",
        "\n",
        "In this section I will present a mathematical approach in which we can estimate the probability to have a topic t, given the fact that we have a new document d, noted p(t/d).\n",
        "\n",
        "t = topic\n",
        "\n",
        "d = document\n",
        "\n",
        "di = word i in document\n",
        "\n",
        "Followin the bayes theorem, we have that:\n",
        "p(t/d) = p(d/t) * p(t) / p(d)\n",
        "\n",
        "At the same time, consdering that the words are independent variables, we can state that p(d/t) = p(d1...dn/t) = prod(p(di, t))\n",
        "\n",
        "The above p(di/t) is actually the word distribution for each topic (phi).\n",
        "\n",
        "p(t) is simply the probability of a topic, which is 1/K.\n",
        "\n",
        "p(d) is the probability of a document, which is 1/(M + 1).\n",
        "\n",
        "Having all the necessary information, it is now fairly simple to compute the desired probability.\n",
        "\n",
        "For computational purpose, I have applied log to the probability and added a small value (epsilon) to each value in phi.\n",
        "\n",
        "log(p(t/d)) = log(p(d/t) + eps) + log(p(t)) - log(p(d))\n",
        "\n",
        "log(p(d/t) + eps) = sum(log(p(di/t) + eps))"
      ],
      "metadata": {
        "id": "N3g0MWGG1Dyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(phi)):\n",
        "    print(\"Topic\", i, \": \", phi[i].value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHVRFWbTHzfa",
        "outputId": "1d9dc2b6-15bc-49a1-bdd3-da01279317fa"
      },
      "execution_count": 319,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0 :  [[0.40098889 0.57504692 0.00780524 0.01615895]]\n",
            "Topic 1 :  [[0.00669411 0.00435578 0.4796438  0.50930632]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_doc = [\"bbb\", \"bbb\", \"aaa\", \"vvv\"]  # make a new document which has 3 words from one topic, and one word from the other\n",
        "new_d = []  # the new document with words translated to numbers\n",
        "for i, d in enumerate(new_doc):\n",
        "    if d in vocab:\n",
        "      new_d.append(vocab[d])\n",
        "\n",
        "print(new_d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEtzzt9PIFZz",
        "outputId": "3afe0067-e3b1-4af2-9f52-ca569bc9934e"
      },
      "execution_count": 320,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 0, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute log(p(d/t))\n",
        "eps = 1e-5\n",
        "def compute_prod_prob(phi, doc, topic):\n",
        "    prob = 0\n",
        "    for di in doc:\n",
        "        prob += log(list(phi[topic].value)[0][di] + eps)\n",
        "    return prob"
      ],
      "metadata": {
        "id": "5cnlV5wXML6B"
      },
      "execution_count": 321,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute log(p(t/d))\n",
        "def compute_prob(p_d_t, p_d, p_t, topic):\n",
        "    return p_d_t + log(p_t) - log(p_d)"
      ],
      "metadata": {
        "id": "Y2aKUCX6NMfO"
      },
      "execution_count": 322,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the most probable topic for the new document\n",
        "def most_prob_topic(new_d):\n",
        "    p_t = 1 / K\n",
        "    p_d = 1 / (M+1)\n",
        "    probs = []\n",
        "    for topic in range(K):\n",
        "        p_d_t = compute_prod_prob(phi, new_d, topic)\n",
        "        p_t_d = compute_prob(p_d_t, p_d, p_t, topic)\n",
        "        probs.append(exp(p_t_d))\n",
        "        print(\"Probability for topic \", topic, \"is\", exp(p_t_d))\n",
        "\n",
        "    max_prob = max(probs)\n",
        "    max_topic = probs.index(max_prob)\n",
        "\n",
        "    print(\"The topic with maximum probability is \", max_topic)\n",
        "\n",
        "most_prob_topic(new_d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrdNntJXJuux",
        "outputId": "61bd5a21-1200-4c4b-e33a-47f621699d28"
      },
      "execution_count": 323,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability for topic  0 is 0.007504376064421666\n",
            "Probability for topic  1 is 2.2778204155982447e-07\n",
            "The topic with maximum probability is  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the document gets assign to the topic that contains 3 out of the 4 words in the document."
      ],
      "metadata": {
        "id": "H_igOFFhrYTl"
      }
    }
  ]
}